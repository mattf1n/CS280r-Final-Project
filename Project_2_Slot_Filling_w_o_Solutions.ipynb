{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 2: Slot Filling w/o Solutions",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL7W88sITA9e",
        "colab_type": "text"
      },
      "source": [
        "# Project 2: Slot Filling\n",
        "\n",
        "*By: Matthew Finlayson & Yuntian Deng*\n",
        "\n",
        "*Based on the original version developed by Matthew Finlayson, Zilin Ma & Emma Rogge.*\n",
        "\n",
        "In Slot filling (sequence labeling), the goal is to label the tokens in a sequence, such as when we want to determine the Part-of-Speech (POS) tag of every word in a sentence, or when we want to recognize the phonmes given a sequence of audio signal. In the last homework, you have built a classification system using Naive Bayes and logistic regression, but in sequence labeling, the token-level classification result not only depends on the observation at that position (captured by emission probability in HMM), but also depends on the context it appears in (modeled by transition probability in HMM).\n",
        "\n",
        "In this homework, you will implement algorithms for slot filling. You will implement both an HMM based approach and a recurrent neural network (RNN) based approach. By the end of this homework, you should have grasped the pros and cons of both approaches.\n",
        "\n",
        "## Goals\n",
        "\n",
        "1. Implement an HMM approach to slot filling.\n",
        "2. Implement an RNN-based approach to slot filling.\n",
        "3. Implement an LSTM-based approach to slot filling.\n",
        "4. Compare the performances of HMM and RNN under different sizes of training data. Discuss the pros and cons of the HMM approach and the neural approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OR8yp3iZ5kl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7P-PwG-XZXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import io\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchtext import datasets, data\n",
        "from torchtext.data import Field\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCcqOSuNjStz",
        "colab_type": "text"
      },
      "source": [
        "#### Mount Data\n",
        "\n",
        "The disk space of hosted runtime would get released after session ends. To avoid losing data, we can mount our Google Drive instead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1J6UPOGjWBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Project2Slot')  #change dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVRNN8aMSosM",
        "colab_type": "text"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "For this project, we will use the ATIS dataset (Airline Travel Information System) taken from Kaggle (https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). In this dataset, we are given questions regarding flight information, and the original task is to answer the questions using a database. However, here we use an intermediary task of POS tagging: given a question, associate each word with its POS. Note that here we use customized POS tags like \"O\" (uninteresting), \"B-round_trip\" (beginning of round trip), \"I-round_trip\" (continuing round trip), etc. Below is an example taken from Kaggle:\n",
        "\n",
        "\n",
        "\n",
        "                              BOS                                        O\n",
        "                         cheapest                          B-cost_relative\n",
        "                          airfare                                        O\n",
        "                             from                                        O\n",
        "                           tacoma                      B-fromloc.city_name\n",
        "                               to                                        O\n",
        "                          orlando                        B-toloc.city_name\n",
        "                              EOS                                        O\n",
        "\n",
        "\n",
        "In this project, our goal is to find the POS tags \"O B-cost_relative O O B-fromloc.city_name O B-toloc.city_name O\" given the sentence \"BOS cheapest airfair from tacoma to orlando EOS\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uk2SI4y2usY",
        "colab_type": "text"
      },
      "source": [
        "First, download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXk30CHiStmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/da03/HarvardCS280/master/atis.train.txt -O atis.train.txt\n",
        "!wget -q https://raw.githubusercontent.com/da03/HarvardCS280/master/atis.dev.txt -O atis.dev.txt\n",
        "!wget -q https://raw.githubusercontent.com/da03/HarvardCS280/master/atis.test.txt -O atis.test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3cNFRmvRgS",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "We use torchtext to load data and convert strings to indicies in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXD8IOqYn6VM",
        "colab_type": "code",
        "outputId": "bb2f9133-f644-4737-966a-440f6e410d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "MIN_FREQ = 3\n",
        "WORD = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", batch_first=True)\n",
        "TAG = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", batch_first=True)\n",
        "fields=(('word', WORD), ('tag', TAG))\n",
        "\n",
        "train, val, test = datasets.SequenceTaggingDataset.splits(\n",
        "            fields=fields, path='./', train='atis.train.txt', validation='atis.dev.txt',\n",
        "            test='atis.test.txt')\n",
        "\n",
        "WORD.build_vocab(train.word, min_freq=MIN_FREQ)\n",
        "TAG.build_vocab(train.tag)\n",
        "\n",
        "print(\"Size of English vocab:\", len(WORD.vocab))\n",
        "print('Most comman English words:', WORD.vocab.freqs.most_common(10))\n",
        "\n",
        "print(\"Size of POS vocab:\", len(TAG.vocab))\n",
        "print('Most comman POS tags:', TAG.vocab.freqs.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of English vocab: 519\n",
            "Most comman English words: [('BOS', 4274), ('EOS', 4274), ('to', 3682), ('from', 3203), ('flights', 2075), ('the', 1745), ('on', 1343), ('flight', 1035), ('me', 1005), ('what', 985)]\n",
            "Size of POS vocab: 105\n",
            "Most comman POS tags: [('O', 38967), ('B-toloc.city_name', 3751), ('B-fromloc.city_name', 3726), ('I-toloc.city_name', 1039), ('B-depart_date.day_name', 835), ('I-fromloc.city_name', 636), ('B-airline_name', 610), ('B-depart_time.period_of_day', 555), ('I-airline_name', 374), ('B-depart_date.day_number', 351)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40owRexuS2e",
        "colab_type": "text"
      },
      "source": [
        "Now, we can iterate over the dataset using torch's iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NssnREWUuZcA",
        "colab_type": "code",
        "outputId": "56a4bdfb-131f-4159-98a3-2cac9de4a4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "BATCH_SIZE = 1 # for simplicity we use batch size 1\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, repeat=False)\n",
        "\n",
        "batch = next(iter(val_iter))\n",
        "\n",
        "print(\"the first batch of words:\", batch.word)\n",
        "print (\"converted to words:\", [WORD.vocab.itos[i] for i in batch.word[0].cpu().tolist()])\n",
        "print(\"the first batch tags\", batch.tag)\n",
        "print (\"converted to tags:\", [TAG.vocab.itos[i] for i in batch.tag[0].cpu().tolist()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the first batch of words: tensor([[  2,   4, 213,   5,   3]])\n",
            "converted to words: ['<bos>', 'BOS', 'airports', 'EOS', '<eos>']\n",
            "the first batch tags tensor([[2, 4, 4, 4, 3]])\n",
            "converted to tags: ['<bos>', 'O', 'O', 'O', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbeIYlULaW8H",
        "colab_type": "text"
      },
      "source": [
        "## Goal 1: HMM for Slot Filling\n",
        "With the iterator, we are ready to implement the HMM and RNN based algorithms for slot filling. First, let's use $Q$ to denote the set of possible POS tags. It is the state space of an HMM, although in our case all states are observed in the training data. Let's use $W$ to denote the vocabulary of all possible words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrjgSjqzxR1J",
        "colab_type": "text"
      },
      "source": [
        "### Learning HMM through counting\n",
        "\n",
        "Recall that an HMM is defined via a transition matrix $A$ which states the probability of moving from one state $q_i$ to another $q_j$ via $a_{ij}=P(q^{t+1}=q_j | q^t=q_i)$, and an emission matrix $B$ which states the probability of generating word $w_i$ given state $q_j$ via $b_{ij}= P(w^t=w_i |q^t= q_j)$. In our case, since all slots are already observed, we can directly use counting to determine $A$ and $B$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfQoO9pVV8zG",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 1 (a): Find the transition matrix**\n",
        "(Jurafsky 8.4.3)\n",
        "\n",
        "The matrix $A$ contains the transition probabilities; $a_ij \\in A$ is the probability of moving from state $q_i \\in Q$ to state $q_j$ such that $\\sum^{n-1}_{j = 0 } a_{ij} = 1$ for all $i$. \n",
        "\n",
        "We find these probabilities by counting the number of times state $q_j$ occurs directly after state $q_i$ divided by the number of times state $q_i$ occurs.\n",
        "\n",
        "$$\n",
        "a_{ij} = \\frac{C(q_iq_j) + 0.01}{C(q_i) + 0.01|Q|}\n",
        "$$\n",
        "\n",
        "In the above formula, we also use LaPlace smoothing to account for the finite size of our training data.\n",
        "\n",
        "Use the above definitions, find the transition matrix $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKkywJsCY5PU",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 1(b): Find the emission matrix $B$**\n",
        "(Jurafsky 8.4.3)\n",
        "\n",
        "Similar to the transition matrix, the emission matrix contains the emission probabilities such that $b_{ij} \\in B$ is probability of the vocabulary word $w_j \\in W$ occuring given that the state is $q_i \\in Q$.\n",
        "\n",
        "We can find this by counting as well.\n",
        "$$\n",
        "P(w_i|q_j) = \\frac{C(w_i, q_j) + 0.01}{C(q_j) + 0.01|W|}\n",
        "$$\n",
        "\n",
        "Again we use LaPlace smoothing to account for the finite size of our training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INnW0jImbK7N",
        "colab_type": "text"
      },
      "source": [
        "### Slot Filling with a learned HMM\n",
        "(Jurafsky 8.4.4)\n",
        "\n",
        "Now that we've already trained an HMM by counting the transition matrix $A$ and the emission matrix $B$, we can apply it to the task of slot filling. Our objective is to find the most probable sequence of slots (parts-of-speech) $\\hat q \\in Q^n$ given a sequence of words $w \\in V^n$.\n",
        "\n",
        "$$\n",
        "\\hat q = \\text{argmax}_{q \\in Q^n}(P(q|w))= \\text{argmax}_{q \\in Q^n}(P(q,w))=\\text{argmax}_{q \\in Q^n}\\left(\\Pi^{n - 1}_{t = 0} P(w^t|q^t)P(q^{t+1}|q^t)\\right)\n",
        "$$\n",
        "where $P(w^t=w_j|q^t=q_i) = b_{ij}$ can be retrieved from our emission matrix $B$, $P(q^{t+1}=q_j|q^t=q_{i})=a_{ij}$ can be retrieved from our transition matrix $A$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xitfy334tTGf",
        "colab_type": "text"
      },
      "source": [
        "##### **Goal 1 (c): Viterbi algorithm**\n",
        "\n",
        "Use dynamic programming to find the most likely sequence for the sequences in the test set and calculate accuracy. Implement the interface viterbi(words, A, B) and compute accuracy using the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIWoUgH7-bb",
        "colab_type": "code",
        "outputId": "f399e612-d992-4dca-f300-7ea3c2cc2fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# compute accuracy given tagging function f(words, A, B)\n",
        "def compute_accuracy(f):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch in test_iter:\n",
        "    words = batch.word[0].cpu().tolist()\n",
        "    tags = batch.tag[0].cpu().tolist()\n",
        "    tags_pred = f(words)\n",
        "    for j, tag in enumerate(tags):\n",
        "      if j == 0 or j == len(tags)-1:\n",
        "        continue\n",
        "      total += 1\n",
        "      if j < len(tags_pred) and tags_pred[j] == tag:\n",
        "        correct += 1\n",
        "    print(\"Accuracy: {}\".format(correct/total))\n",
        "\n",
        "  #print(\"Accuracy: {}\".format(correct/total))\n",
        "  return correct/total\n",
        "\n",
        "def viterbi(words):\n",
        "  # TODO: replace baseline with your implementation here\n",
        "  return [TAG.vocab.stoi['O'] for _ in words]\n",
        "\n",
        "compute_accuracy(viterbi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.6842105263157895\n",
            "Accuracy: 0.68\n",
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.6623376623376623\n",
            "Accuracy: 0.6526315789473685\n",
            "Accuracy: 0.6403508771929824\n",
            "Accuracy: 0.6573426573426573\n",
            "Accuracy: 0.6573426573426573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6573426573426573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjFo1OArzi8k",
        "colab_type": "text"
      },
      "source": [
        "## Goal 2: RNN for Slot Filling\n",
        "HMMs work pretty well for this slot filling task. Now let's take an alternative (and more popular) approach: RNN (LSTM) based sequence tagging. Similar to the HMM part of this project, you will need to both learn a model using training data, and decode using a trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwedn2mA4CQK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![RNN Visualization](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png)\n",
        "The recurrent neural network is simple, after unfolding it. Each subsequent cell takes the output of the previous cell using the following equations. This can be done in Pytorch by using modules such as torch.nn.linear.\n",
        "\n",
        "$$\n",
        "h_t = \\sigma_h (W_h x_t + U_h h_{t-1} + b_h)\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_t = \\sigma_y (W_y h_t + b_y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9d85vSvC9sd",
        "colab_type": "text"
      },
      "source": [
        "### Learning RNN through back-propagation\n",
        "\n",
        "To learn an RNN for sequence tagging, we will compute loss based on training data, and then back-propagate to compute the gradients with respect to its parameters. Finally, we will update the parameters along the direction of the negative gradients to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwwyTH0gD6wm",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 2 (a): RNN training**\n",
        "\n",
        "Implement the forward pass of the RNN tagger and the loss function using the below starter code. The training/optimization code is already provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6UqQaNME1Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "#       RNN       #\n",
        "###################\n",
        "\n",
        "class RNNTagger(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, \n",
        "                embedding_dim = 36, lr = 0.0003):\n",
        "    super(RNNTagger, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    # TODO: implement below\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # TODO: implement below\n",
        "    return output, hidden\n",
        "\n",
        "  def compute_loss(output, ground_truth):\n",
        "    # TODO: implement below\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def train_epoch(self, train_iter, val_iter, epochs = 3, lr = 0.001):\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "    self.train()\n",
        "    best_ppl = float('inf')\n",
        "    for epoch in range(epochs): \n",
        "      print(\"epoch:\", epoch)\n",
        "      for batch in train_iter:\n",
        "        words = batch.word[0]\n",
        "        tags = batch.tag[0]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        self.zero_grad()\n",
        "\n",
        "        # Step 2. Run our forward pass.\n",
        "        hidden = torch.zeros(1, self.hidden_size)\n",
        "        loss = 0\n",
        "        for word, tag in zip(words, tags):\n",
        "          output, hidden = self.forward(word.view(-1), hidden)\n",
        "          loss = loss + compute_loss(output, tag.view(-1))\n",
        "        # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      val_ppl = self.validate(val_iter)\n",
        "      print ('Epoch:', epoch, \"PPL:\", val_ppl)\n",
        "      if val_ppl < best_ppl:\n",
        "        best_ppl = val_ppl\n",
        "        best_state_dict = copy.deepcopy(self.state_dict())\n",
        "    # use the model with the best val ppl\n",
        "    self.load_state_dict(best_state_dict)\n",
        "\n",
        "  def validate(self, val_iter):\n",
        "    self.eval()\n",
        "    # we don't want to change the gradient while validating. \n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in val_iter:\n",
        "        words = batch.word[0]\n",
        "        tags = batch.tag[0]\n",
        "\n",
        "        # Step 1. Run our forward pass.\n",
        "        hidden = torch.zeros(1, self.hidden_size)\n",
        "        loss = 0\n",
        "        for word, tag in zip(words, tags):\n",
        "          output, hidden = self(word.view(-1), hidden)\n",
        "          loss = loss + self.loss_function(output, tag.view(-1))\n",
        "        tot_count += len(words)\n",
        "        tot_loss += loss\n",
        "      ppl = math.exp(tot_loss / tot_count)\n",
        "      return ppl\n",
        "\n",
        "\n",
        "rnn = RNNTagger(len(WORD.vocab), hidden_size = 36, output_size = len(TAG.vocab), embedding_dim = 36)\n",
        "rnn.train_epoch(train_iter, val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4AfuG9t5mt3",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 2 (b) RNN decoding**\n",
        "\n",
        "With a trained RNN, implement rnnDecode to tag the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN9_wpjK52yJ",
        "colab_type": "code",
        "outputId": "1f4424cf-af60-4525-ce5a-839f84a4ba9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def rnnDecode(words):\n",
        "  # Step 2. Run our forward pass.\n",
        "  hidden = torch.zeros(1, rnn.hidden_size)\n",
        "  loss = 0\n",
        "  tags = []\n",
        "  for word in torch.LongTensor(words):\n",
        "    # TODO: replace below with your implementation\n",
        "    tags.append(TAG.vocab.stoi['O'])\n",
        "  return tags\n",
        "compute_accuracy(rnnDecode)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6827210177852785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCpYjn9W4yNK",
        "colab_type": "text"
      },
      "source": [
        "## Goal 3: LSTM for Slot Filliing\n",
        "Did your RNN perform better at the task than HMM? How much better was it? Was it expected? RNN has a vanishing gradient problem. https://en.wikipedia.org/wiki/Vanishing_gradient_problem To solve this, Long-Short Term Memory was introduced. The full implementation is simplified to just calling LSTM() from pytorch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE705g_XBCpD",
        "colab_type": "text"
      },
      "source": [
        "### LSTM training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OyA79q-76QG",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 3 (a) Use LSTM instead of RNN**\n",
        "\n",
        "Use LSTM instead of RNN to do slot filling, implement the LSTMTagger interface with the below starter code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPhkh18_BLc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "#       LSTM      #\n",
        "###################\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, \n",
        "                embedding_dim = 36, lr = 0.0003):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    # TODO: implement below\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # TODO: implement below\n",
        "    return output, hidden\n",
        "\n",
        "  def compute_loss(self, output, ground_truth):\n",
        "    # TODO: implement below\n",
        "    return loss\n",
        "\n",
        "  def train_epoch(self, train_iter, val_iter, epochs = 3, lr = 0.001):\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "    self.train()\n",
        "    best_ppl = float('inf')\n",
        "    for epoch in range(epochs): \n",
        "      print(\"epoch:\", epoch)\n",
        "      for batch in train_iter:\n",
        "        words = batch.word[0]\n",
        "        tags = batch.tag[0]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        self.zero_grad()\n",
        "\n",
        "        # Step 2. Run our forward pass.\n",
        "        hidden = torch.zeros(1, 1, self.hidden_size)\n",
        "        hidden = (hidden, hidden)\n",
        "        loss = 0\n",
        "        for word, tag in zip(words, tags):\n",
        "          output, hidden = self(word.view(-1), hidden)\n",
        "          loss = loss + compute_loss(output, tag.view(-1))\n",
        "        # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "      val_ppl = self.validate(val_iter)\n",
        "      print ('Epoch:', epoch, \"PPL:\", val_ppl)\n",
        "      if val_ppl < best_ppl:\n",
        "        best_ppl = val_ppl\n",
        "        best_state_dict = copy.deepcopy(self.state_dict())\n",
        "    # use the model with the best val ppl\n",
        "    self.load_state_dict(best_state_dict)\n",
        "\n",
        "  def validate(self, val_iter):\n",
        "    self.eval()\n",
        "    # we don't want to change the gradient while validating. \n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in val_iter:\n",
        "        words = batch.word[0]\n",
        "        tags = batch.tag[0]\n",
        "\n",
        "        # Step 1. Run our forward pass.\n",
        "        hidden = torch.zeros(1, 1, self.hidden_size)\n",
        "        hidden = (hidden, hidden)\n",
        "        loss = 0\n",
        "        for word, tag in zip(words, tags):\n",
        "          output, hidden = self.forward(word.view(-1), hidden)\n",
        "          loss = loss + self.loss_function(output, tag.view(-1))\n",
        "        tot_count += len(words)\n",
        "        tot_loss += loss\n",
        "      ppl = math.exp(tot_loss / tot_count)\n",
        "      return ppl\n",
        "\n",
        "\n",
        "lstm = LSTMTagger(len(WORD.vocab), hidden_size = 36, output_size = len(TAG.vocab), embedding_dim = 36)\n",
        "lstm.train_epoch(train_iter, val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHJH0T4hAmg8",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Decoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fdcORrHAsFg",
        "colab_type": "text"
      },
      "source": [
        "#### **Goal 3 (b) Use LSTM for decoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llx63LPWKpgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstmDecode(words):\n",
        "  # Step 2. Run our forward pass.\n",
        "  hidden = torch.zeros(1, 1, lstm.hidden_size)\n",
        "  hidden = (hidden, hidden)\n",
        "  loss = 0\n",
        "  tags = []\n",
        "  for word in torch.LongTensor(words):\n",
        "    # TODO: replace below with your implementation\n",
        "    tag = TAG.vocab.stoi['O']\n",
        "    tags.append(tag.item())\n",
        "  return tags\n",
        "compute_accuracy(lstmDecode)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ3eziD0CEgw",
        "colab_type": "text"
      },
      "source": [
        "## **Goal 4: Compare HMM to RNN/LSTM under different amount of training data**\n",
        "\n",
        "Vary the amount of training data, compare the performance of HMM to RNN/LSTM. Discuss what are the pros and cons of HMM and RNN/LSTM? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3H5F0-9emu_",
        "colab_type": "text"
      },
      "source": [
        "The below code shows how to subsample the training set with downsample ratio `ratio`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulq-AP_tdzBu",
        "colab_type": "code",
        "outputId": "e4cf21c8-ddf0-400e-93ab-eb097def66a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "ratio = 0.1\n",
        "MIN_FREQ = 3\n",
        "WORD = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", batch_first=True)\n",
        "TAG = data.Field(init_token=\"<bos>\", eos_token=\"<eos>\", batch_first=True)\n",
        "fields=(('word', WORD), ('tag', TAG))\n",
        "\n",
        "train, val, test = datasets.SequenceTaggingDataset.splits(\n",
        "            fields=fields, path='./', train='atis.train.txt', validation='atis.dev.txt',\n",
        "            test='atis.test.txt')\n",
        "\n",
        "random.shuffle(train.examples)\n",
        "train.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\n",
        "WORD.build_vocab(train.word, min_freq=MIN_FREQ)\n",
        "TAG.build_vocab(train.tag)\n",
        "\n",
        "print(\"Size of English vocab:\", len(WORD.vocab))\n",
        "print('Most comman English words:', WORD.vocab.freqs.most_common(10))\n",
        "\n",
        "print(\"Size of POS vocab:\", len(TAG.vocab))\n",
        "print('Most comman POS tags:', TAG.vocab.freqs.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of English vocab: 219\n",
            "Most comman English words: [('BOS', 427), ('EOS', 427), ('to', 348), ('from', 311), ('flights', 205), ('the', 168), ('on', 132), ('flight', 104), ('me', 97), ('show', 91)]\n",
            "Size of POS vocab: 86\n",
            "Most comman POS tags: [('O', 3835), ('B-toloc.city_name', 373), ('B-fromloc.city_name', 360), ('I-toloc.city_name', 102), ('B-depart_date.day_name', 77), ('I-fromloc.city_name', 71), ('B-depart_time.period_of_day', 65), ('B-airline_name', 55), ('B-depart_date.month_name', 37), ('B-depart_date.day_number', 37)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}